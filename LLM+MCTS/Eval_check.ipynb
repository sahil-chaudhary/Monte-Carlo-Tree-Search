{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-DVG9lQbDED4pAGLLojr5T3BlbkFJAOrjyHbmt3WyfO2siJWW\"\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set your OpenAI API key\n",
    "\n",
    "\n",
    "# Load the Olympiadbench dataset\n",
    "dataset = load_dataset(\"openai/Olympiadbench\")\n",
    "\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"your_api_key\"\n",
    "\n",
    "# Load the Olympiadbench dataset\n",
    "# Load the saved dataset from mcts-r processed data\n",
    "dataset = load_dataset(\"openai/Olympiadbench\")\n",
    "\n",
    "\n",
    "def evaluate(model, prompt, reference):\n",
    "  # Use the OpenAI API to generate a response with GPT-3.5 Turbo\n",
    "  response = openai.Completion.create(\n",
    "      engine=\"text-davinci-003\",  # Replace with \"text-davinci-003\" for GPT-3.5 Turbo\n",
    "      prompt=prompt,\n",
    "      max_tokens=100,  # Adjust for desired response length\n",
    "      n=1,\n",
    "      stop=None,\n",
    "      temperature=0.7\n",
    "  )\n",
    "  generated_response = response.choices[0].text.strip()\n",
    "\n",
    "  # Calculate metrics\n",
    "  # 1. Accuracy (percentage of characters that match exactly)\n",
    "  num_correct = sum(c1 == c2 for c1, c2 in zip(reference, generated_response))\n",
    "  accuracy = num_correct / len(reference) * 100\n",
    "\n",
    "  # 2. Precision (ratio of correctly predicted positive cases)\n",
    "  # Identify true positives (correctly generated characters in the reference)\n",
    "  true_positives = sum(1 for c in generated_response if c in reference)\n",
    "  # Identify all predicted characters (all characters in generated_response)\n",
    "  predicted_positives = len(generated_response)\n",
    "  precision = (true_positives / predicted_positives) if predicted_positives > 0 else 0\n",
    "\n",
    "  # 3. Recall (ratio of correctly identified relevant characters)\n",
    "  # Identify true positives (correctly generated characters in the reference)\n",
    "  true_positives = sum(1 for c in generated_response if c in reference)\n",
    "  # Identify all relevant characters (all characters in the reference)\n",
    "  relevant_characters = len(reference)\n",
    "  recall = (true_positives / relevant_characters) if relevant_characters > 0 else 0\n",
    "\n",
    "  # 4. F1 score (harmonic mean of precision and recall)\n",
    "  f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "  return {\n",
    "      \"accuracy\": accuracy,\n",
    "      \"precision\": precision,\n",
    "      \"recall\": recall,\n",
    "      \"f1\": f1\n",
    "  }\n",
    "\n",
    "\n",
    "# Choose the model to evaluate\n",
    "model_name = \"text-davinci-003\"  # Example: GPT-3.5 Turbo\n",
    "\n",
    "# Create a pipeline for easier model interaction\n",
    "model = pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "# Initialize empty dictionaries to store evaluation results\n",
    "all_metrics = {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "# Iterate through the dataset\n",
    "for datapoint in dataset:\n",
    "  prompt = datapoint[\"prompt\"]\n",
    "  reference = datapoint[\"reference\"]\n",
    "  # Evaluate on the current datapoint\n",
    "  metrics = evaluate(model, prompt, reference)\n",
    "  # Accumulate evaluation results\n",
    "  for metric_name, value in metrics.items():\n",
    "    all_metrics[metric_name].append(value)\n",
    "\n",
    "# Calculate overall average scores for each metric\n",
    "for metric_name, scores in all_metrics.items():\n",
    "  average_score = sum(scores) / len(scores)\n",
    "  print(f\"Average {metric_name}: {average_score:.4f}\")\n",
    "\n",
    "# Calculate overall average scores for each metric\n",
    "for metric_name, scores in all_metrics.items():\n",
    "  average_score = sum(scores) / len(scores)\n",
    "  print(f\"Average {metric_name}: {average_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from datasets import load_dataset\n",
    "import sympy  # Library for symbolic math operations\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"your_api_key\"\n",
    "\n",
    "# Load the Olympiadbench dataset\n",
    "dataset = load_dataset(\"openai/Olympiadbench\")\n",
    "\n",
    "\n",
    "def evaluate(model, prompt, reference):\n",
    "  # Use the OpenAI API to answer the math question\n",
    "  response = openai.Completion.create(\n",
    "      engine=\"text-davinci-003\",  # Replace with \"text-davinci-003\" for GPT-3.5 Turbo\n",
    "      prompt=prompt,\n",
    "      max_tokens=100,  # Adjust for desired response length\n",
    "      n=1,\n",
    "      stop=None,\n",
    "      temperature=0.7\n",
    "  )\n",
    "  generated_response = response.choices[0].text.strip()\n",
    "\n",
    "  # Evaluate answer (considering symbolic representation)\n",
    "  try:\n",
    "    # Parse the reference answer (assuming it's a mathematical expression)\n",
    "    reference_answer = sympy.sympify(reference)\n",
    "    # Parse the generated response (assuming it's a mathematical expression)\n",
    "    generated_answer = sympy.sympify(generated_response)\n",
    "    # Check if the answers are symbolically equivalent\n",
    "    answer_correct = sympy.simplify(reference_answer - generated_answer) == 0\n",
    "  except (sympy.SympifyError, ValueError):\n",
    "    # Handle cases where parsing fails (e.g., non-mathematical responses)\n",
    "    answer_correct = False\n",
    "\n",
    "  # Calculate accuracy (1 if answers are symbolically equivalent, 0 otherwise)\n",
    "  accuracy = 1 if answer_correct else 0\n",
    "\n",
    "  # Additional metrics (precision, recall, F1) might not be applicable here)\n",
    "  # For these tasks, accuracy might be sufficient.\n",
    "\n",
    "  return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# Choose the model to evaluate\n",
    "model_name = \"text-davinci-003\"  # Example: GPT-3.5 Turbo\n",
    "\n",
    "# Create a pipeline for easier model interaction (not strictly necessary here)\n",
    "# model = pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "# Initialize empty dictionary to store evaluation results\n",
    "all_metrics = {\"accuracy\": []}\n",
    "\n",
    "# Iterate through the dataset\n",
    "for datapoint in dataset:\n",
    "  prompt = datapoint[\"prompt\"]\n",
    "  reference = datapoint[\"reference\"]\n",
    "  # Evaluate on the current datapoint\n",
    "  metrics = evaluate(model, prompt, reference)\n",
    "  # Accumulate evaluation results\n",
    "  for metric_name, value in metrics.items():\n",
    "    all_metrics[metric_name].append(value)\n",
    "\n",
    "# Calculate overall average score for accuracy\n",
    "average_accuracy = sum(all_metrics[\"accuracy\"]) / len(all_metrics[\"accuracy\"])\n",
    "print(f\"Average Accuracy: {average_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
