{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahil-chaudhary/Monte-Carlo-Tree-Search/blob/main/LLaMA_3B_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am not sure why we need to clone the original repo"
      ],
      "metadata": {
        "id": "8AqYwB4mglVx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93kczI8NPbzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e69ccc-ff03-4722-c087-580148cc9609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: retry in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry) (4.4.2)\n",
            "Requirement already satisfied: py<2.0.0,>=1.4.26 in /usr/local/lib/python3.10/dist-packages (from retry) (1.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "fatal: destination path 'mathblackbox' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets numpy retry openai\n",
        "!git clone https://github.com/trotsky1997/mathblackbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H3l6_irVjVO"
      },
      "source": [
        "Import the dependencies and select model for MCTS-r\n",
        "\n",
        "Different datasets used in the original papers and pre-cleaned and processed. Try datasets which are downloadable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgqX24ohuTzb"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from curses.ascii import isalpha, isdigit\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "import re\n",
        "import socket\n",
        "import sys\n",
        "from datasets import load_dataset\n",
        "import hashlib\n",
        "import json\n",
        "import random\n",
        "from functools import lru_cache\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from openai import OpenAI\n",
        "from retry import retry\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# MODEL_NAME = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "# MODEL_NAME  = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "MODEL_NAME  = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
        "# MODEL_NAME = 'google/gemma-1.1-7b-it'\n",
        "# MODEL_NAME = 'test-lora'\n",
        "# MODEL_NAME = '/home/bingxing2/ailab/group/ai4phys/EXPORT/new_mistral_7b_4'\n",
        "# MODEL_NAME = ''\n",
        "\n",
        "\n",
        "# DATA_NAME = 'meta-math-40k-pathfinder-mistral7B'\n",
        "# DATA_NAME = 'meta-math-40k-pathfinder-llama2_7B'\n",
        "# DATA_NAME = 'meta-math-40k-testtime-llama2_7B'\n",
        "# DATA_NAME = 'gsm8k-rs-llama2_7B'\n",
        "# DATA_NAME = 'meta-math-40k-testtime-mistral7B'\n",
        "# DATA_NAME = 'gsm8k-rs-mistral7B'\n",
        "# DATA_NAME = 'gsm8k-sample-testtime-mistral-dpo-7'\n",
        "# DATA_NAME = 'gsm8k-testtime-mistral_7B_pathfinder_0'\n",
        "# DATA_NAME = 'MATH-rs-mistral7B'\n",
        "# DATA_NAME = 'gsm8k-pathfinder-gemma7b-new-mcts-8'\n",
        "\n",
        "# DATA_NAME = 'gsmhard-pathfinder-llama3-8b-new-mcts-8'\n",
        "# DATA_NAME = 'olympiadbench-pathfinder-llama3-8b-new-mcts-8'\n",
        "# DATA_NAME = 'GAIC-pathfinder-llama3-8b-new-mcts-8'\n",
        "# DATA_NAME = 'MATH-pathfinder-llama3-8b-new-mcts-8'\n",
        "# DATA_NAME = 'AIME-pathfinder-llama3-8b-mcts-2'\n",
        "# DATA_NAME = 'gsm8k-testtime-pathfinder-mistral7B-mcts-2'\n",
        "# DATA_NAME = 'gsm8k-testtime-pathfinder-pureseq-mistral7B-5'\n",
        "DATA_NAME = 'olympiadbench'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHhoFUD8VjVQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52DBJ0pBucoe"
      },
      "outputs": [],
      "source": [
        "if MODEL_NAME == '':\n",
        "    MODEL_NAME = sys.argv[1]\n",
        "\n",
        "if DATA_NAME == '':\n",
        "    DATA_NAME = sys.argv[2]\n",
        "\n",
        "def last_boxed_only_string(string):\n",
        "    idx = string.rfind('\\\\boxed')\n",
        "    if idx < 0:\n",
        "        idx = string.rfind('\\\\fbox')\n",
        "        if idx < 0:\n",
        "            return None\n",
        "\n",
        "    i = idx\n",
        "    right_brace_idx = None\n",
        "    num_left_braces_open = 0\n",
        "    while i < len(string):\n",
        "        if string[i] == '{':\n",
        "            num_left_braces_open += 1\n",
        "        if string[i] == '}':\n",
        "            num_left_braces_open -= 1\n",
        "            if num_left_braces_open == 0:\n",
        "                right_brace_idx = i\n",
        "                break\n",
        "        i += 1\n",
        "\n",
        "    if right_brace_idx is None:\n",
        "        retval = None\n",
        "    else:\n",
        "        retval = string[idx:right_brace_idx + 1]\n",
        "\n",
        "    return retval\n",
        "\n",
        "\n",
        "def remove_boxed(s):\n",
        "    left = '\\\\boxed{'\n",
        "    try:\n",
        "        assert s[:len(left)] == left\n",
        "        assert s[-1] == '}'\n",
        "        return s[len(left):-1]\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_boxed_answer(pred_str, strip_double_curly_brace=False):\n",
        "    boxed_str = last_boxed_only_string(pred_str)\n",
        "    if boxed_str is None:\n",
        "        return None\n",
        "    answer = remove_boxed(boxed_str)\n",
        "    if answer is None:\n",
        "        return None\n",
        "    if strip_double_curly_brace:\n",
        "        match = re.match(r'^\\{(.*)\\}$', answer)\n",
        "        # match = re.match('^\\{(.*)\\}$', answer)  # noqa: W605\n",
        "        if match:\n",
        "            answer = match.group(1)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the datasets which is going to be used for training\n",
        "\n",
        "although, We have selected less number of datasets because it takes a lot of time for training"
      ],
      "metadata": {
        "id": "8UC5uCC8uRpB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9HexfoAVjVQ"
      },
      "outputs": [],
      "source": [
        "if 'testtime' in DATA_NAME:\n",
        "    if 'gsm8k' in DATA_NAME:\n",
        "        if 'sample' in DATA_NAME:\n",
        "            dataset = load_dataset(\"gsm8k\",'main',split='test')\n",
        "            # dataset = dataset.shuffle()\n",
        "            dataset = dataset.select(range(10))\n",
        "        else:\n",
        "            dataset = load_dataset(\"gsm8k\",'main',split='test')\n",
        "    elif 'MATH' in DATA_NAME:\n",
        "        dataset = load_dataset(\"lighteval/MATH\",'all',split='test')\n",
        "else:\n",
        "    if 'gsmhard' in DATA_NAME:\n",
        "        dataset = load_dataset(\"reasoning-machines/gsm-hard\",split='train')\n",
        "    elif 'gsm8k' in DATA_NAME:\n",
        "        if not 'mcts' in DATA_NAME:\n",
        "            dataset = load_dataset(\"gsm8k\",'main',split='train')\n",
        "        else:\n",
        "            dataset = load_dataset(\"gsm8k\",'main',split='test')\n",
        "    elif 'level5' in DATA_NAME:\n",
        "        dataset = load_dataset(\"lighteval/MATH\",'all',split='test',trust_remote_code=True)\n",
        "        dataset = dataset.filter(lambda example: example[\"level\"].endswith(\"5\"))\n",
        "    elif 'MATH' in DATA_NAME and not'level5' in DATA_NAME:\n",
        "        dataset = load_dataset(\"lighteval/MATH\",'all',split='test',trust_remote_code=True)\n",
        "    elif 'AIME' in DATA_NAME:\n",
        "        dataset = load_dataset(\"qq8933/AIME_1983_2024\",split='train')\n",
        "    elif 'olympiadbench' in DATA_NAME:\n",
        "        dataset = load_dataset(\"lmms-lab/OlympiadBench\",split='test_en')\n",
        "        dataset = dataset.filter(lambda example:len(example[\"images\"]) == 0 and example['final_answer'] is not None and len(example['final_answer']) == 1)\n",
        "    elif 'meta-math' in DATA_NAME:\n",
        "        dataset = load_dataset(\"meta-math/MetaMathQA-40K\",split='train')\n",
        "    elif 'GAIC' in DATA_NAME:\n",
        "        dataset = load_dataset(\"qq8933/AGI_Odyssey_MATH_GAIC_2024\")\n",
        "    elif 'mathinstruct' in DATA_NAME:\n",
        "        dataset = load_dataset('TIGER-Lab/MathInstruct',split='train')\n",
        "    else:\n",
        "        dataset = load_dataset('json',data_files=f'/home/bingxing2/ailab/group/ai4phys/math/data_mistral_var_sft.json')\n",
        "\n",
        "\n",
        "# Reduce the number of dataset to 10 for now. Just for testing\n",
        "\n",
        "dataset = dataset.select(range(1))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution cleaning\n",
        "Here, mostly answer extraction is done and rewritten in a better format"
      ],
      "metadata": {
        "id": "4Q7U8qdmCO6M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CVijYuAvEvZ"
      },
      "outputs": [],
      "source": [
        "class Extractor:\n",
        "\n",
        "    def extract_matching_bracket(cls, target_str: str):\n",
        "        if not target_str:\n",
        "            return target_str\n",
        "        current_nest_level = 1\n",
        "        for i, ch in enumerate(target_str):\n",
        "            if ch == '{':\n",
        "                current_nest_level += 1\n",
        "            elif ch == '}':\n",
        "                current_nest_level -= 1\n",
        "            if current_nest_level == 0:\n",
        "                break\n",
        "        return target_str[:i]\n",
        "\n",
        "    def clean(cls, target_str: str):\n",
        "        opt = target_str.strip().replace('{{', '{').replace('}}', '}')\n",
        "        if not opt:\n",
        "            return opt\n",
        "        if opt[-1] == '.' or opt[-1] == '。':\n",
        "            return opt[:-1]\n",
        "        return opt\n",
        "\n",
        "    def extract_answer(cls, pred: str, extract_last_num=False):\n",
        "        ## does this idea work to extract exact answer??\n",
        "        if pred.find('The final answer is ') >= 0:\n",
        "            x = pred[pred.find('The final answer is ') +\n",
        "                     len('The final answer is '):]\n",
        "            x = x[1:x.find('$.')]\n",
        "            # print(x)\n",
        "            return cls.clean(x)\n",
        "        if pred.find('\\n\\nQuestion:') >= 0:\n",
        "            pred = pred.split('\\n\\nQuestion:')[0]\n",
        "            if pred.find('The answer is'):\n",
        "                pred = pred[pred.find('The answer is') + len('The answer is'):]\n",
        "                return cls.clean(pred)\n",
        "        if pred.find('# Answer') >= 0:\n",
        "            return cls.clean(pred[pred.find('# Answer') + len('# Answer'):])\n",
        "        if pred.find('The answer is:') >= 0:\n",
        "            return cls.clean(pred[pred.find('The answer is:') +\n",
        "                                  len('The answer is:'):])\n",
        "        if pred.find('####') >= 0:\n",
        "            return cls.clean(pred[pred.find('####') + 4:])\n",
        "        left = '\\\\boxed{'\n",
        "        if pred.find(left) >= 0:\n",
        "            pred = pred[pred.find(left) + len(left):]\n",
        "            return cls.clean(cls.extract_matching_bracket(pred))\n",
        "\n",
        "        if extract_last_num:\n",
        "            nums = []\n",
        "            opt = ''\n",
        "\n",
        "            def contain_digit(opt):\n",
        "                for ch in opt:\n",
        "                    if ch.isdigit():\n",
        "                        return True\n",
        "                return False\n",
        "\n",
        "            for ch in pred:\n",
        "                if ch.isdigit() or ch in ' ,.':\n",
        "                    opt = opt + ch\n",
        "                else:\n",
        "                    if contain_digit(opt):\n",
        "                        nums.append(opt)\n",
        "                    opt = ''\n",
        "            if contain_digit(opt):\n",
        "                return cls.clean(opt)\n",
        "            if nums:\n",
        "                return cls.clean(nums[-1])\n",
        "        return None\n",
        "\n",
        "\n",
        "def fix_fracs(string):\n",
        "    substrs = string.split('\\\\frac')\n",
        "    new_str = substrs[0]\n",
        "    if len(substrs) > 1:\n",
        "        substrs = substrs[1:]\n",
        "        for substr in substrs:\n",
        "            new_str += '\\\\frac'\n",
        "            if substr[0] == '{':\n",
        "                new_str += substr\n",
        "            else:\n",
        "                try:\n",
        "                    assert len(substr) >= 2\n",
        "                except AssertionError:\n",
        "                    return string\n",
        "                a = substr[0]\n",
        "                b = substr[1]\n",
        "                if b != '{':\n",
        "                    if len(substr) > 2:\n",
        "                        post_substr = substr[2:]\n",
        "                        new_str += '{' + a + '}{' + b + '}' + post_substr\n",
        "                    else:\n",
        "                        new_str += '{' + a + '}{' + b + '}'\n",
        "                else:\n",
        "                    if len(substr) > 2:\n",
        "                        post_substr = substr[2:]\n",
        "                        new_str += '{' + a + '}' + b + post_substr\n",
        "                    else:\n",
        "                        new_str += '{' + a + '}' + b\n",
        "    string = new_str\n",
        "    return string\n",
        "\n",
        "\n",
        "def fix_a_slash_b(string):\n",
        "    if len(string.split('/')) != 2:\n",
        "        return string\n",
        "    a = string.split('/')[0]\n",
        "    b = string.split('/')[1]\n",
        "    try:\n",
        "        a = int(a)\n",
        "        b = int(b)\n",
        "        assert string == '{}/{}'.format(a, b)\n",
        "        new_string = '\\\\frac{' + str(a) + '}{' + str(b) + '}'\n",
        "        return new_string\n",
        "    except AssertionError:\n",
        "        return string\n",
        "\n",
        "\n",
        "def remove_right_units(string):\n",
        "    # \"\\\\text{ \" only ever occurs (at least in the val set)\n",
        "    if '\\\\text{ ' in string:\n",
        "        splits = string.split('\\\\text{ ')\n",
        "        assert len(splits) == 2\n",
        "        return splits[0]\n",
        "    else:\n",
        "        return string\n",
        "\n",
        "\n",
        "def fix_sqrt(string):\n",
        "    if '\\\\sqrt' not in string:\n",
        "        return string\n",
        "    splits = string.split('\\\\sqrt')\n",
        "    new_string = splits[0]\n",
        "    for split in splits[1:]:\n",
        "        if split[0] != '{':\n",
        "            a = split[0]\n",
        "            new_substr = '\\\\sqrt{' + a + '}' + split[1:]\n",
        "        else:\n",
        "            new_substr = '\\\\sqrt' + split\n",
        "        new_string += new_substr\n",
        "    return new_string\n",
        "\n",
        "\n",
        "def strip_string(string):\n",
        "    # linebreaks\n",
        "    string = string.replace('\\n', '')\n",
        "\n",
        "    # remove inverse spaces\n",
        "    string = string.replace('\\\\!', '')\n",
        "\n",
        "    # replace \\\\ with \\\n",
        "    string = string.replace('\\\\\\\\', '\\\\')\n",
        "\n",
        "    # replace tfrac and dfrac with frac\n",
        "    string = string.replace('tfrac', 'frac')\n",
        "    string = string.replace('dfrac', 'frac')\n",
        "\n",
        "    # remove \\left and \\right\n",
        "    string = string.replace('\\\\left', '')\n",
        "    string = string.replace('\\\\right', '')\n",
        "\n",
        "    # Remove circ (degrees)\n",
        "    string = string.replace('^{\\\\circ}', '')\n",
        "    string = string.replace('^\\\\circ', '')\n",
        "\n",
        "    # remove dollar signs\n",
        "    string = string.replace('\\\\$', '')\n",
        "\n",
        "    # remove units (on the right)\n",
        "    string = remove_right_units(string)\n",
        "\n",
        "    # remove percentage\n",
        "    string = string.replace('\\\\%', '')\n",
        "    # string = string.replace('\\%', '')  # noqa: W605\n",
        "    string = string.replace('%', '')  # noqa: W605\n",
        "\n",
        "    string = string.replace(' .', ' 0.')\n",
        "    string = string.replace('{.', '{0.')\n",
        "    # if empty, return empty string\n",
        "    if len(string) == 0:\n",
        "        return string\n",
        "    if string[0] == '.':\n",
        "        string = '0' + string\n",
        "\n",
        "    # to consider: get rid of e.g. \"k = \" or \"q = \" at beginning\n",
        "    if len(string.split('=')) == 2:\n",
        "        if len(string.split('=')[0]) <= 2:\n",
        "            string = string.split('=')[1]\n",
        "\n",
        "    # fix sqrt3 --> sqrt{3}\n",
        "    string = fix_sqrt(string)\n",
        "\n",
        "    # remove spaces\n",
        "    string = string.replace(' ', '')\n",
        "\n",
        "    string = fix_fracs(string)\n",
        "\n",
        "    # manually change 0.5 --> \\frac{1}{2}\n",
        "    if string == '0.5':\n",
        "        string = '\\\\frac{1}{2}'\n",
        "\n",
        "    string = fix_a_slash_b(string)\n",
        "    string = string.replace('x \\\\in', '').strip()  # noqa: W605\n",
        "\n",
        "    # a_b == a, a_{b} == a_b for bit conversion\n",
        "    if string.find('_') >= 0:\n",
        "        p = string.split('_')\n",
        "        p[1] = p[1].replace('{', '').replace('}', '')\n",
        "        string = '_'.join(p)\n",
        "\n",
        "    # 10800 == 10,800; we only deal with single number\n",
        "    if string.strip().find(' ') == -1 and string.find('(') == -1:\n",
        "        string = string.replace(',', '')\n",
        "\n",
        "    return string\n",
        "\n",
        "\n",
        "def is_equiv(str1, str2, verbose=False):\n",
        "    if str1 is None and str2 is None:\n",
        "        # print(\"WARNING: Both None\")\n",
        "        return False\n",
        "    if str1 is None or str2 is None:\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        ss1 = strip_string(str1)\n",
        "        ss2 = strip_string(str2)\n",
        "        return ss1 == ss2\n",
        "    except Exception:\n",
        "        return str1 == str2\n",
        "\n",
        "\n",
        "#if not os.path.exists(DATA_NAME):\n",
        "#    os.mkdir(DATA_NAME)\n",
        "#if not os.path.exists(f'{DATA_NAME}/jsons'):\n",
        "#    os.mkdir(f'{DATA_NAME}/jsons')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response generating and creating a Tree\n",
        "The cell work is pretty simple to understand, generates responses for each query and measures the correctness based on reward model and accuracy is measured using different distance function.\n",
        "\n",
        "Trees are also created as suggested in the algorithm and measurement of UCB value and update ucb for exploration is done here."
      ],
      "metadata": {
        "id": "cnwKVCr9DEvN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGEaiaxJPhuX"
      },
      "outputs": [],
      "source": [
        "dataset.shuffle()\n",
        "clients = []\n",
        "times = time.time()\n",
        "\n",
        "# def create_client(api_key=\"sk-8093342a3add41e391be36b60472b299\"):\n",
        "#     \"\"\"Creates a new OpenAI client object with proper error handling and return value.\n",
        "\n",
        "#     Args:\n",
        "#         api_key (str, optional): Your OpenAI API key. Defaults to \"sk-8093342a3add41e391be36b60472b299\".\n",
        "\n",
        "#     Returns:\n",
        "#         openai.Client: The created OpenAI client object if successful.\n",
        "#         None: If an error occurs during client creation.\n",
        "#     \"\"\"\n",
        "\n",
        "#     try:\n",
        "#         client = openai.Client(\n",
        "#             # base_url=f\"https://api.deepseek.com/\",\n",
        "#             api_key=\"api_key\"\n",
        "#         )\n",
        "\n",
        "#         # Test the connection (optional)\n",
        "#         client.ping()  # Raises an exception if there's an issue\n",
        "\n",
        "#         return client\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error creating client: {e}\")\n",
        "#         return None\n",
        "\n",
        "def create_client():\n",
        "  \"\"\"Creates a new OpenAI clinet object with error handling\"\"\"\n",
        "    client = OpenAI(\n",
        "    # base_url=f\"https://api.deepseek.com/\",\n",
        "    api_key=\"api_key\",\n",
        "    )\n",
        "    print(client)\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "     messages=[\n",
        "            {\"role\": \"user\", \"content\": 'hi'}#+'\\nBe concisely and clearly in no more than 50 words.'\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(completion.choices[0].message)\n",
        "    try:\n",
        "        client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": 'hi'}#+'\\nBe concisely and clearly in no more than 50 words.'\n",
        "        ],\n",
        "        # max_tokens=min(len(prompt)+128,8000),\n",
        "        temperature=0.95,#0.5 if 'testtime' in DATA_NAME else random.uniform(0,1)\n",
        "        timeout=15\n",
        "        )\n",
        "        print(len(clients)+1)\n",
        "        return client\n",
        "    except:\n",
        "        return client\n",
        "\n",
        "# @retry()\n",
        "def generate(prompt,history=[],timeout = 150,truncate=True):\n",
        "  \"\"\"The function generates the responses for each prompt/query raised in the datasets\"\"\"\n",
        "    if 'testtime' in DATA_NAME:\n",
        "        timeout=150\n",
        "    print('awaiting response...')\n",
        "\n",
        "    time0 = time.time()\n",
        "    history_ = [{\"role\": \"user\" if i %2 ==0 else 'assistant', \"content\": h} for i,h in enumerate(history)]\n",
        "    if truncate:\n",
        "        history_ = history_[-2:]\n",
        "    completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=history_+[\n",
        "                # dict(role='user', content=\"Question: Angelo and Melanie want to plan how many hours over the next week they should study together for their test next week. They have 2 chapters of their textbook to study and 4 worksheets to memorize. They figure out that they should dedicate 3 hours to each chapter of their textbook and 1.5 hours for each worksheet. If they plan to study no more than 4 hours each day, how many days should they plan to study total over the next week if they take a 10-minute break every hour, include 3 10-minute snack breaks each day, and 30 minutes for lunch each day?\\nLet's think step by step\\nAnswer:\"),\n",
        "                # dict(role='assistant', content=\"Angelo and Melanie think they should dedicate 3 hours to each of the 2 chapters, 3 hours x 2 chapters = 6 hours total.\\nFor the worksheets they plan to dedicate 1.5 hours for each worksheet, 1.5 hours x 4 worksheets = 6 hours total.\\nAngelo and Melanie need to start with planning 12 hours to study, at 4 hours a day, 12 / 4 = 3 days.\\nHowever, they need to include time for breaks and lunch. Every hour they want to include a 10-minute break, so 12 total hours x 10 minutes = 120 extra minutes for breaks.\\nThey also want to include 3 10-minute snack breaks, 3 x 10 minutes = 30 minutes.\\nAnd they want to include 30 minutes for lunch each day, so 120 minutes for breaks + 30 minutes for snack breaks + 30 minutes for lunch = 180 minutes, or 180 / 60 minutes per hour = 3 extra hours.\\nSo Angelo and Melanie want to plan 12 hours to study + 3 hours of breaks = 15 hours total.\\nThey want to study no more than 4 hours each day, 15 hours / 4 hours each day = 3.75\\nThey will need to plan to study 4 days to allow for all the time they need.\\nThe answer is 4\\n\"),\n",
        "                # dict(role='user', content=\"Question: Mark's basketball team scores 25 2 pointers, 8 3 pointers and 10 free throws.  Their opponents score double the 2 pointers but half the 3 pointers and free throws.  What's the total number of points scored by both teams added together?\\nLet's think step by step\\nAnswer:\"),\n",
        "                # dict(role='assistant', content=\"Mark's team scores 25 2 pointers, meaning they scored 25*2= 50 points in 2 pointers.\\nHis team also scores 6 3 pointers, meaning they scored 8*3= 24 points in 3 pointers\\nThey scored 10 free throws, and free throws count as one point so they scored 10*1=10 points in free throws.\\nAll together his team scored 50+24+10= 84 points\\nMark's opponents scored double his team's number of 2 pointers, meaning they scored 50*2=100 points in 2 pointers.\\nHis opponents scored half his team's number of 3 pointers, meaning they scored 24/2= 12 points in 3 pointers.\\nThey also scored half Mark's team's points in free throws, meaning they scored 10/2=5 points in free throws.\\nAll together Mark's opponents scored 100+12+5=117 points\\nThe total score for the game is both team's scores added together, so it is 84+117=201 points\\nThe answer is 201\\n\"),\n",
        "                # dict(role='user', content=\"Question: Bella has two times as many marbles as frisbees. She also has 20 more frisbees than deck cards. If she buys 2/5 times more of each item, what would be the total number of the items she will have if she currently has 60 marbles?\\nLet's think step by step\\nAnswer:\"),\n",
        "                # dict(role='assistant', content=\"When Bella buys 2/5 times more marbles, she'll have increased the number of marbles by 2/5*60 = 24\\nThe total number of marbles she'll have is 60+24 = 84\\nIf Bella currently has 60 marbles, and she has two times as many marbles as frisbees, she has 60/2 = 30 frisbees.\\nIf Bella buys 2/5 times more frisbees, she'll have 2/5*30 = 12 more frisbees.\\nThe total number of frisbees she'll have will increase to 30+12 = 42\\nBella also has 20 more frisbees than deck cards, meaning she has 30-20 = 10 deck cards\\nIf she buys 2/5 times more deck cards, she'll have 2/5*10 = 4 more deck cards.\\nThe total number of deck cards she'll have is 10+4 = 14\\nTogether, Bella will have a total of 14+42+84 = 140 items\\nThe answer is 140\\n\"),\n",
        "                # dict(role='user', content=\"Question: A group of 4 fruit baskets contains 9 apples, 15 oranges, and 14 bananas in the first three baskets and 2 less of each fruit in the fourth basket. How many fruits are there?\\nLet's think step by step\\nAnswer:\"),\n",
        "                # dict(role='assistant', content=\"For the first three baskets, the number of apples and oranges in one basket is 9+15=24\\nIn total, together with bananas, the number of fruits in one basket is 24+14=38 for the first three baskets.\\nSince there are three baskets each having 38 fruits, there are 3*38=114 fruits in the first three baskets.\\nThe number of apples in the fourth basket is 9-2=7\\nThere are also 15-2=13 oranges in the fourth basket\\nThe combined number of oranges and apples in the fourth basket is 13+7=20\\nThe fourth basket also contains 14-2=12 bananas.\\nIn total, the fourth basket has 20+12=32 fruits.\\nThe four baskets together have 32+114=146 fruits.\\nThe answer is 146\\n\"),\n",
        "        {\"role\": \"user\", \"content\": prompt}#\n",
        "    ],\n",
        "    # max_tokens=min(len(prompt)+128,8000),\n",
        "    temperature=0.5,#0.5 if 'testtime' in DATA_NAME else random.uniform(0,1),\n",
        "    timeout = timeout\n",
        "    )\n",
        "\n",
        "    print('done')\n",
        "    print(f'response received! time taken: {time.time()-time0} seconds.')\n",
        "    return completion.choices[0].message.content,list(history)+[prompt,completion.choices[0].message.content]\n",
        "\n",
        "# @retry()\n",
        "def cal_reward(question,ans):\n",
        "  \"\"\"Function generates a reward by raising a query to analyze the answer\"\"\"\n",
        "    query = f'Question: {question}\\nAnswer:{ans}\\nAnalyze this Answer Strictly and Critic, point out every flaw for ervery possible imperfect to minus every possible score! You need to be very harsh and mean in calculating grades, and never give full marks to ensure that the marks are authoritative. \\nOutput a score between [-100,+100], ig. from -100 to +100. \\nResponse format:\\n[Analyst]...[Score]...'\n",
        "    ret = generate(query)\n",
        "    score = ret[0].split('Score')[-1]\n",
        "    scores = pattern.findall(score)\n",
        "    if not scores:\n",
        "        raise Exception('no')\n",
        "    else:\n",
        "        ret = float(scores[-1])\n",
        "        # if abs(ret - 100.0) < 1e-5:\n",
        "        #     ret = 50.0\n",
        "        if ret >= 95:\n",
        "            ret = 50\n",
        "        # elif ret <= -100:\n",
        "        #     ret = -50\n",
        "        return ret\n",
        "\n",
        "# @retry()\n",
        "def get_weak_answer(question,new_len=0,ans_format=''):\n",
        "    query = f'Question: {question}\\nThe response should begin with [reasoning process]...[Verification]... and end with {ans_format}\\nLet\\'s think step by step.'\n",
        "    return generate(query,timeout=90)\n",
        "\n",
        "def get_weak_hints(question,weak_answer,ground_truth_label=None,new_len=0,history=[],alreadygood=False,ans_format=''):\n",
        "    query = f'Question: {question}\\nSince we have a weak Answer, could you provide me with a relection or feedback to correct this answer better? Analyze this Answer Strictly and Critic, point out every flaw for ervery possible imperfect to minus every possible score!\\nLet\\'s think step by step.'\n",
        "    return generate(query,history)\n",
        "\n",
        "def get_better_answer(question,weak_answer,hint,new_len=0,history=[],ans_format=''):\n",
        "    query = f'Question: {question}\\nPlease refine the your answer according to your Reflection or Feedback. The response should begin with [reasoning process]...[Verification]... and end with end with {ans_format}\\nLet\\'s think step by step.'\n",
        "    return generate(query,history)\n",
        "\n",
        "def get_gt_hints(question,ground_truth,new_len=0):\n",
        "    query = f\"Question: {question}\\nGround Truth:{ground_truth}\\nAccording to ground truth answer we have, Could you descript the thought process of ground truth answer, please don’t give me the answer, just the thought process?\"\n",
        "    return generate(query)\n",
        "\n",
        "\n",
        "datas = []\n",
        "pattern = re.compile(r'\\-?\\d+\\.\\d+|\\-?\\d+')\n",
        "extractor_0 = Extractor()\n",
        "@lru_cache(1024)\n",
        "def extract_label(text: str,type='') -> str:\n",
        "    if 'gsm' not in DATA_NAME and type != 'digit':\n",
        "        if '####' in text:\n",
        "            text = text.split('####')[-1]\n",
        "        elif 'The answer is' in text:\n",
        "            text = text.split('The answer is')[-1]\n",
        "            if '####' in text:\n",
        "                text = text.split('####')[-1]\n",
        "        if 'box' in text:\n",
        "            return extract_boxed_answer(text)\n",
        "        else:\n",
        "            return text\n",
        "    if '\\n####' in text:\n",
        "        text = text.split('\\n####')[-1].replace(',','')\n",
        "    elif 'The answer is' in text:\n",
        "        text = text.split('The answer is')[-1].replace(',','')\n",
        "    numbers = pattern.findall(text)\n",
        "    if not numbers:\n",
        "        return None\n",
        "    if '\\n####' in text or 'The answer is' in text:\n",
        "        return numbers[0]\n",
        "    else :\n",
        "        return numbers[-1]\n",
        "\n",
        "@lru_cache(1024)\n",
        "def check(gt,ans):\n",
        "    gt_label = extract_label(gt)\n",
        "    if gt_label.isdigit():\n",
        "        type = 'digit'\n",
        "    elif gt_label.isupper() and gt_label.isalpha():\n",
        "        type = 'option'\n",
        "    elif gt_label.lower() in ['yes','no']:\n",
        "        gt_label = gt_label.lower()\n",
        "        type = 'yesorno'\n",
        "    else :\n",
        "        type = 'formula'\n",
        "    ans_label = extract_label(ans,type)\n",
        "    if ans_label:\n",
        "        if type == 'option':\n",
        "            ans_label = ans_label.strip()[0]\n",
        "        elif type == 'yesorno':\n",
        "            ans_label = ans_label.lower()\n",
        "        elif type == 'formula':\n",
        "            ans_label = ans_label.replace('$','')\n",
        "    print(gt_label,ans_label)\n",
        "    if 'gsm' not in DATA_NAME and type != 'digit':\n",
        "        return is_equiv(gt_label,ans_label)\n",
        "    print(gt_label,ans_label)\n",
        "    if gt_label is None or ans_label is None:\n",
        "        return False\n",
        "    if ans_label == gt_label or abs(float(ans_label) - float(gt_label)) < 1e-5:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def hamming_distance(str1, str2):\n",
        "    if len(str1) != len(str2):\n",
        "        raise ValueError(\"Strings must be of the same length\")\n",
        "    return sum(el1 != el2 for el1, el2 in zip(str1[::-1], str2[::-1]))\n",
        "\n",
        "def simple_reward(gt,ans):\n",
        "    gt_f = format(float(extract_label(gt)),'.5f')\n",
        "    ans_f = format(float(extract_label(ans)),'.5f')\n",
        "    return -hamming_distance(gt_f,ans_f)\n",
        "\n",
        "def sort_answers_and_rewards(answers, rewards):\n",
        "    # Zip answers and rewards together\n",
        "    answer_reward_pairs = zip(answers, rewards)\n",
        "\n",
        "    # Sort pairs by rewards\n",
        "    sorted_pairs = sorted(answer_reward_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Extract sorted answers and rewards\n",
        "    sorted_answers = [pair[0] for pair in sorted_pairs]\n",
        "    sorted_rewards = [pair[1] for pair in sorted_pairs]\n",
        "\n",
        "    return sorted_answers, sorted_rewards\n",
        "\n",
        "def filter_mature_node(childs, to_explore, to_explore_reward,max_expand=3):\n",
        "    filterd_to_explore = []\n",
        "    avg_reward = {node: (min(to_explore_reward[node]) + np.mean(to_explore_reward[node])) / 2 for node in to_explore}\n",
        "\n",
        "    for node in to_explore:\n",
        "        if len(childs.get(node,[])) < max_expand or max([avg_reward.get(child,-999) for child in childs.get(node,[])]) < avg_reward.get(node,-999):\n",
        "            filterd_to_explore.append(node)\n",
        "\n",
        "    return filterd_to_explore\n",
        "\n",
        "\n",
        "def get_best_explore_from_ucb(to_explore, ucb_bank):\n",
        "    # 初始化最佳节点和最高UCB值\n",
        "    # Comments by Sahil and Balaji,RedTree\n",
        "    #Initialize Exploration from Upper connfidence bound\n",
        "    best_node = None\n",
        "    highest_ucb = float('-inf')\n",
        "\n",
        "    # 遍历所有待探索的节点\n",
        "    for node in to_explore:\n",
        "        ucb_value = ucb_bank.get(node, float('-inf'))\n",
        "        if ucb_value > highest_ucb:\n",
        "            highest_ucb = ucb_value\n",
        "            best_node = node\n",
        "\n",
        "    return best_node\n",
        "\n",
        "\n",
        "def compute_ucb(r_c, N_n, N_c, C):\n",
        "    # Comments by Sahil and Balaji, Redtree\n",
        "    # Function as mentioned in the paper while epsiolon value is close to 1e-5\n",
        "    return r_c + C * math.sqrt(math.log(N_n + 1) / (N_c + 1e-5))\n",
        "\n",
        "def update_ucb(fathers, childs, to_explore, to_explore_reward, ucb_bank, C=1.4,gamma=0.85):\n",
        "    # 计算所有节点的访问次数\n",
        "    visit_count = {node: len(to_explore_reward[node]) for node in to_explore}\n",
        "\n",
        "    # 计算所有节点的平均奖励\n",
        "    # avg_reward = {node: sum(to_explore_reward[node]) / len(to_explore_reward[node]) for node in to_explore}\n",
        "    avg_reward = {node: (min(to_explore_reward[node]) + np.mean(to_explore_reward[node])) / 2 for node in to_explore}\n",
        "\n",
        "    # 获取所有叶子节点\n",
        "    leaves = set(to_explore) - set(fathers.values())\n",
        "\n",
        "    # 更新所有叶子节点的UCB值\n",
        "    for leaf in leaves:\n",
        "        # ucb_bank[leaf] = avg_reward[leaf]\n",
        "        ucb_bank[leaf] = compute_ucb(avg_reward[leaf],len(to_explore_reward.get(fathers.get(leaf,None),[])),len(to_explore_reward.get(leaf,[])),C)\n",
        "\n",
        "    # 从叶子节点向上更新父节点的UCB值\n",
        "    nodes_to_update = list(leaves)\n",
        "    while nodes_to_update:\n",
        "        new_nodes_to_update = set()\n",
        "        for node in nodes_to_update:\n",
        "            father = fathers.get(node)\n",
        "            if father is not None:\n",
        "                if father not in ucb_bank:\n",
        "                    new_nodes_to_update.add(father)\n",
        "                if father in ucb_bank:\n",
        "                    # 计算父节点的UCB值\n",
        "                    ucb_values = []\n",
        "                    child_reward = []\n",
        "                    for child in childs[father]:\n",
        "                        ucb_values.append(ucb_bank[child])\n",
        "                        child_reward.append(avg_reward[child])\n",
        "                    father_reward = (avg_reward[father] + max(child_reward))/2\n",
        "                    ucb_bank[father] = compute_ucb(father_reward,len(to_explore_reward.get(fathers.get(father,None),[])),len(to_explore_reward.get(father,[])),C)\n",
        "        nodes_to_update = list(new_nodes_to_update)\n",
        "\n",
        "def step(query,weak_answer,ground_truth_label=None,history=[],alreadygood=False,ans_format=''):\n",
        "    hints,history = get_weak_hints(query,weak_answer,ground_truth_label=ground_truth_label,history=history,alreadygood=alreadygood,ans_format=ans_format)\n",
        "    answer,history = get_better_answer(query,weak_answer,hints,history=history,ans_format=ans_format)\n",
        "    return hints,answer,history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Loop and Conditions for Terminations\n",
        "\n",
        "Functions in this cell are the point of execution for everything. This can be used to create pipeline for any models.\n",
        "\n",
        "For now, the termination case is limited to max iter to 5 but can be increased as per the result. Although, I believe there can be another better method of looking for convergence in the solutions and stop instead of relying on max-iter for termination."
      ],
      "metadata": {
        "id": "IqyI9kTUDno3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amcOOJN2vdQw"
      },
      "outputs": [],
      "source": [
        "def main_loop(query,ground_truth,max_iter=8,ans_format=''):\n",
        "    to_explore = []\n",
        "    to_explore_reward = {}\n",
        "    history_bank = {}\n",
        "    hints_bank = {}\n",
        "    ucb_bank = {}\n",
        "    fathers = {}\n",
        "    childs = {}\n",
        "    def sampling_reward(answer):\n",
        "        if answer not in to_explore_reward:\n",
        "            to_explore_reward[answer] = []\n",
        "        reward = cal_reward(query,answer)\n",
        "        # if check(ground_truth,answer):\n",
        "        #     reward += 100\n",
        "        to_explore_reward[answer].append(reward)\n",
        "\n",
        "    def add_to_hints_bank(hints,weak_answer):\n",
        "        if weak_answer not in hints_bank:\n",
        "            hints_bank[weak_answer] = []\n",
        "        hints_bank[weak_answer].append(hints)\n",
        "\n",
        "    def add_to_childs(father,child):\n",
        "        if father not in childs:\n",
        "            childs[father] = []\n",
        "        childs[father].append(child)\n",
        "\n",
        "    hints_reward_imp_bank = {}\n",
        "    def add_to_hints_reward_imp_bank(hints,weak_answer,reward,answer):\n",
        "        if weak_answer not in hints_reward_imp_bank:\n",
        "            hints_reward_imp_bank[weak_answer] = []\n",
        "        hints_reward_imp_bank[weak_answer].append((hints,reward,answer))\n",
        "    ground_truth_label = extract_label(ground_truth)\n",
        "    ###get weak answer###\n",
        "    weak_answer,history = get_weak_answer(query,ans_format=ans_format)\n",
        "    history_bank[weak_answer] = tuple(history)\n",
        "    answers_list = [weak_answer,]\n",
        "    to_explore = [weak_answer,]\n",
        "    childs[weak_answer] = []\n",
        "    fathers[weak_answer] = None\n",
        "    # to_explore_reward = [cal_reward(query,weak_answer),]\n",
        "    sampling_reward(weak_answer)\n",
        "    ##add total-bad answer###\n",
        "    # if check(ground_truth,weak_answer):\n",
        "    #     return\n",
        "    if True:#not check(ground_truth,weak_answer):\n",
        "        total_bad = random.choice([\"I Don't Know\",\"I can't understand this question.\",\"I can't help with this question.\",\"I don't know how to solve this question.\",\"I don't know the answer to this question.\",\"I don't know the answer to this question, sorry.\"])\n",
        "        total_bad_history = copy.deepcopy(history)\n",
        "        total_bad_history[-1] = total_bad\n",
        "        history_bank[total_bad] = tuple(total_bad_history)\n",
        "        answers_list += [total_bad,]\n",
        "        to_explore += [total_bad,]\n",
        "        childs[total_bad] = []\n",
        "        fathers[total_bad] = None\n",
        "        # to_explore_reward = [cal_reward(query,weak_answer),]\n",
        "        sampling_reward(total_bad)\n",
        "    hints_list = []\n",
        "    if check(ground_truth,weak_answer) :#and 'testtime' in DATA_NAME\n",
        "        return hints_list,answers_list,to_explore,to_explore_reward,hints_bank,history_bank,hints_reward_imp_bank,fathers,childs,ucb_bank\n",
        "    patient = 0 if 'testtime' not in DATA_NAME else 0\n",
        "    alpha = 0.45\n",
        "    update_ucb(fathers=fathers,childs=childs,to_explore=to_explore,to_explore_reward=to_explore_reward,ucb_bank=ucb_bank)\n",
        "    for i in range(max_iter):\n",
        "        print('iteration:',i)\n",
        "        filterd_to_explore = filter_mature_node(childs, to_explore, to_explore_reward)\n",
        "        weak_answer = get_best_explore_from_ucb(filterd_to_explore, ucb_bank)\n",
        "        sampling_reward(weak_answer)\n",
        "        hints,answer,history = step(query,weak_answer,history=history_bank[weak_answer],ans_format=ans_format)\n",
        "        add_to_hints_bank(hints,weak_answer)\n",
        "        history_bank[answer] = tuple(history)\n",
        "        to_explore.append(answer)\n",
        "        sampling_reward(answer)\n",
        "        fathers[answer] = weak_answer\n",
        "        childs[answer] = []\n",
        "        add_to_childs(weak_answer,answer)\n",
        "        answers_list.append(answer)\n",
        "        hints_list.append(hints)\n",
        "        if check(ground_truth,answer) and 'testtime' in DATA_NAME:\n",
        "            return hints_list,answers_list,to_explore,to_explore_reward,hints_bank,history_bank,hints_reward_imp_bank,fathers,childs,ucb_bank\n",
        "        elif check(ground_truth,answer) and 'testtime' not in DATA_NAME:\n",
        "            if patient <= 0:\n",
        "                return hints_list,answers_list,to_explore,to_explore_reward,hints_bank,history_bank,hints_reward_imp_bank,fathers,childs,ucb_bank\n",
        "            patient -= 1\n",
        "        update_ucb(fathers=fathers,childs=childs,to_explore=to_explore,to_explore_reward=to_explore_reward,ucb_bank=ucb_bank)\n",
        "        add_to_hints_reward_imp_bank(hints,weak_answer,min(to_explore_reward.get(answer)) - min(to_explore_reward.get(weak_answer)),answer)#ucb_bank[answer] - ucb_bank[weak_answer]\n",
        "    return hints_list,answers_list,to_explore,to_explore_reward,hints_bank,history_bank,hints_reward_imp_bank,fathers,childs,ucb_bank\n",
        "\n",
        "def tryfunc(example):\n",
        "    try:\n",
        "        if os.path.exists(f'{DATA_NAME}/jsons/{hashlib.md5(str(example).encode()).hexdigest()}.json.lock'):\n",
        "            return\n",
        "        else:\n",
        "            os.system(f'touch {DATA_NAME}/jsons/{hashlib.md5(str(example).encode()).hexdigest()}.json.lock')\n",
        "        func(example)\n",
        "        if os.path.exists(f'{DATA_NAME}/jsons/{hashlib.md5(str(example).encode()).hexdigest()}.json.lock'):\n",
        "            os.system(f'rm {DATA_NAME}/jsons/{hashlib.md5(str(example).encode()).hexdigest()}.json.lock')\n",
        "    except:\n",
        "        print(example)\n",
        "        pass\n",
        "    # for example in tqdm(dataset['train']):\n",
        "\n",
        "def func(example):\n",
        "    if os.path.exists(f'{DATA_NAME}/jsons/{hashlib.md5(str(example).encode()).hexdigest()}.json'):\n",
        "        # return json.load(open(f'{DATA_NAME}/jsons/{hashlib.md5(str(example).encode()).hexdigest()}'))\n",
        "        return {}\n",
        "    if 'instruction' in example and 'output' in example:\n",
        "        query = example['instruction'] + '\\n' + example['input']\n",
        "        ground_truth = example['output']\n",
        "    elif 'context' in example and 'question' in example:\n",
        "        if example['context']:\n",
        "            query = example['context'] + '\\n' + example['question']\n",
        "        else:\n",
        "            query = example['question']\n",
        "        ground_truth = example['final_answer'][0].replace('$','')\n",
        "    elif 'GAIC' in DATA_NAME :\n",
        "        query = example['problem']\n",
        "        ground_truth = example['answer']\n",
        "    else:\n",
        "        if 'query' in example:\n",
        "            query = example['query']\n",
        "        elif 'problem' in example:\n",
        "            query = example['problem']\n",
        "        elif 'input' in example:\n",
        "            query = example['input']\n",
        "        elif 'Question' in example:\n",
        "            query = example['Question']\n",
        "        else:\n",
        "            query = example['question']\n",
        "        if 'response' in example:\n",
        "            ground_truth = example['response']\n",
        "        elif 'solution' in example:\n",
        "            ground_truth = example['solution']\n",
        "        elif 'target' in example:\n",
        "            ground_truth = str(example['target'])\n",
        "        elif 'Answer' in example:\n",
        "            ground_truth = example['Answer']\n",
        "        else:\n",
        "            ground_truth = example['answer']\n",
        "\n",
        "    if 'gsm' in DATA_NAME:\n",
        "        ans_format = r'\"[Final Answer] The answer is [answer] \\n#### [answer]\"'\n",
        "    else:\n",
        "        if extract_label(ground_truth).isdigit():\n",
        "            ans_format = r'\"[Final Answer] The answer is [number] \\n#### [number]\"'\n",
        "        elif extract_label(ground_truth).isalpha() and extract_label(ground_truth).isupper():\n",
        "            ans_format = r'\"[Final Answer] The answer is \\\\boxed{[option]} \\n#### [option]\"'\n",
        "        elif extract_label(ground_truth).lower() in ['yes','no']:\n",
        "            ans_format = r'\"[Final Answer] The answer is \\\\boxed{[Yes or No]} \\n#### [Yes or No]\"'\n",
        "        else:\n",
        "            ans_format = r'\"[Final Answer] The answer is \\\\boxed{[answer formula]} \\n#### [answer formula]\"'\n",
        "\n",
        "    # new_len = len(ground_truth)\n",
        "    hints_prompt = f'Question: {query}\\nCould you provide me with the thought process to solve this problem, but please don’t give me the answer or calculation, just the thought process?'\n",
        "    max_iter = 5\n",
        "    if 'meta-math' in DATA_NAME:\n",
        "        max_iter = 8\n",
        "    if 'testtime' in DATA_NAME:\n",
        "        max_iter = 2\n",
        "    hints_list,answers_list,to_explore,to_explore_reward,hints_bank,history_bank,hints_reward_imp_bank,fathers,childs,ucb_bank = main_loop(query,ground_truth,max_iter=max_iter,ans_format=ans_format)\n",
        "    if len(answers_list) <= 1 and 'rs' in DATA_NAME:\n",
        "        return\n",
        "    else:\n",
        "        if not 'testtime' in DATA_NAME:\n",
        "            # gt_hints = get_gt_hints(query,ground_truth)\n",
        "            gt_hints = ''\n",
        "            pass\n",
        "        else:\n",
        "            gt_hints = ''\n",
        "        data = {\n",
        "            'query':query,\n",
        "            'ground_truth':ground_truth,\n",
        "            'hints_list':hints_list,\n",
        "            'answers_list':answers_list,\n",
        "            'ground_truth_hints':gt_hints,\n",
        "            'hints_prompt':hints_prompt,\n",
        "            'to_explore':to_explore,\n",
        "            'to_explore_reward': {k: [float(v) for v in values] for k, values in to_explore_reward.items()},\n",
        "            'hints_bank':hints_bank,\n",
        "            'history_bank':history_bank,\n",
        "            'hints_reward_imp_bank':hints_reward_imp_bank,\n",
        "            'fathers':fathers,\n",
        "            'childs':childs,\n",
        "            'ucb_bank':ucb_bank,\n",
        "        }\n",
        "        if 'rs' in DATA_NAME and not check(ground_truth,answers_list[-1]):\n",
        "            return\n",
        "\n",
        "        os.makedirs(f'{DATA_NAME}/jsons', exist_ok=True)\n",
        "\n",
        "        with open(f'{DATA_NAME}/jsons/{hashlib.md5(str(example).encode()).hexdigest()}.json','w+') as f:\n",
        "            json.dump(data,f,indent=4,ensure_ascii=False)\n",
        "\n",
        "        return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAYoumKRuqX9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "261gXS3PSqWA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "8240640a6b5d4b0a8351f70fa50e982e",
            "a4a69e477c824a79b806f470513dada5",
            "3bd479db34354cc581abc8d3d9799dea",
            "05b437a063bd41b38a10e52fcc792cae",
            "9b3ea5e3a0f34413b1c1c61855c3d7ec",
            "5d18824453c14f23b52f6eba8f2e30ba",
            "3583fde4d8d44acbb595d86b0a8b5371",
            "3017fc4d713244e09b8450758f5c7ecf",
            "23142c8f172f4f4a9ddbdf04691af1fa",
            "b285afdab61141309c0bec8848de0344",
            "f0ffc4bed663449fa8fd048bec3c9e9a",
            "57682b1d6048456bbafcd4918224186b",
            "59d56324e02b45d681b22a20582d6882",
            "df92d17306ea4f069eacee3c4747650b",
            "497e3b01bef2402e9e227d52d4b40858",
            "9f43aff2ef6444ebae20f01e2f91d114",
            "bb31f0f159d7416684f88fb1fc16643f",
            "c98cf064266b4adab337f288479b314e",
            "c18f77ee882b43158db65e202a597a37",
            "d023f38c27fc4961a8263ab1d993bb08",
            "6f857d8f41a74bc5a8fa3cf0c53eed78",
            "63efe2822bfb409c9019690df4344d78"
          ]
        },
        "outputId": "d7601988-087e-4b6d-a78c-40955bc4d03e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<openai.OpenAI object at 0x7c59ac0e36d0>\n",
            "ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None)\n",
            "1\n",
            "<openai.OpenAI object at 0x7c59ac0e36d0>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8240640a6b5d4b0a8351f70fa50e982e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57682b1d6048456bbafcd4918224186b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    # while True:\n",
        "    #     try:\n",
        "    # datas = dataset.map(func,num_proc=len(clients)*8)\n",
        "    client = create_client()\n",
        "    print(client)\n",
        "    datas = dataset.map(func,num_proc=1)\n",
        "        # except :\n",
        "        #     continue\n",
        "        # break\n",
        "    datas.save_to_disk('openai-coder')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8240640a6b5d4b0a8351f70fa50e982e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4a69e477c824a79b806f470513dada5",
              "IPY_MODEL_3bd479db34354cc581abc8d3d9799dea",
              "IPY_MODEL_05b437a063bd41b38a10e52fcc792cae"
            ],
            "layout": "IPY_MODEL_9b3ea5e3a0f34413b1c1c61855c3d7ec"
          }
        },
        "a4a69e477c824a79b806f470513dada5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d18824453c14f23b52f6eba8f2e30ba",
            "placeholder": "​",
            "style": "IPY_MODEL_3583fde4d8d44acbb595d86b0a8b5371",
            "value": "Map: 100%"
          }
        },
        "3bd479db34354cc581abc8d3d9799dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3017fc4d713244e09b8450758f5c7ecf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23142c8f172f4f4a9ddbdf04691af1fa",
            "value": 1
          }
        },
        "05b437a063bd41b38a10e52fcc792cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b285afdab61141309c0bec8848de0344",
            "placeholder": "​",
            "style": "IPY_MODEL_f0ffc4bed663449fa8fd048bec3c9e9a",
            "value": " 1/1 [00:00&lt;00:00, 19.97 examples/s]"
          }
        },
        "9b3ea5e3a0f34413b1c1c61855c3d7ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d18824453c14f23b52f6eba8f2e30ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3583fde4d8d44acbb595d86b0a8b5371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3017fc4d713244e09b8450758f5c7ecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23142c8f172f4f4a9ddbdf04691af1fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b285afdab61141309c0bec8848de0344": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0ffc4bed663449fa8fd048bec3c9e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57682b1d6048456bbafcd4918224186b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59d56324e02b45d681b22a20582d6882",
              "IPY_MODEL_df92d17306ea4f069eacee3c4747650b",
              "IPY_MODEL_497e3b01bef2402e9e227d52d4b40858"
            ],
            "layout": "IPY_MODEL_9f43aff2ef6444ebae20f01e2f91d114"
          }
        },
        "59d56324e02b45d681b22a20582d6882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb31f0f159d7416684f88fb1fc16643f",
            "placeholder": "​",
            "style": "IPY_MODEL_c98cf064266b4adab337f288479b314e",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "df92d17306ea4f069eacee3c4747650b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c18f77ee882b43158db65e202a597a37",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d023f38c27fc4961a8263ab1d993bb08",
            "value": 1
          }
        },
        "497e3b01bef2402e9e227d52d4b40858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f857d8f41a74bc5a8fa3cf0c53eed78",
            "placeholder": "​",
            "style": "IPY_MODEL_63efe2822bfb409c9019690df4344d78",
            "value": " 1/1 [00:00&lt;00:00, 22.10 examples/s]"
          }
        },
        "9f43aff2ef6444ebae20f01e2f91d114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb31f0f159d7416684f88fb1fc16643f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c98cf064266b4adab337f288479b314e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c18f77ee882b43158db65e202a597a37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d023f38c27fc4961a8263ab1d993bb08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f857d8f41a74bc5a8fa3cf0c53eed78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63efe2822bfb409c9019690df4344d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}